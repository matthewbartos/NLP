{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inżynieria lingwistyczna\n",
    "Ten notebook jest oceniany półautomatycznie. Nie twórz ani nie usuwaj komórek - struktura notebooka musi zostać zachowana. Odpowiedź wypełnij tam gdzie jest na to wskazane miejsce - odpowiedzi w innych miejscach nie będą sprawdzane (nie są widoczne dla sprawdzającego w systemie).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 1 - tokenizacja (12 pkt)\n",
    "\n",
    "Jedną z nowoczesnych technik tokenizacji jest BPE - byte-pair encoding [1]. Technika ta polega na podzielenie słów na częste podsłowa (morfemy). W przeciwieństwie do podejść lingwistycznych, wymagających reguł tworzenia morfemów, BPE wyznacza je automatycznie poprzez wyznaczenie najczęstszych przylegających do siebie sekwencji znaków które występują obok siebie.\n",
    "\n",
    "Algorytm przebiega w następujących krokach.\n",
    "1. Podziel wszystkie słowa na symbole (początkowo pojedyncze znaki)\n",
    "2. Wyznacz najczęściej występującą obok siebie parę symboli \n",
    "3. Stwórz nowy symbol będący konkatenacją dwóch najczęstszych symboli.\n",
    "\n",
    "Uwaga 1: każde słowo zakończone jest specjalnym symbolem końca wyrazu.\n",
    "\n",
    "Uwaga 2: tworzenie nowego symbolu nie powoduje usuniecie starego tj. zawsze jednym z możliwych symboli jest pojedynczy znak, ale jeśli można to stosujemy symbol dłuższy.\n",
    "\n",
    "Przykład: korpus w którym występuje ,,ala'' 5 razy i ,,mama 10 razy''\n",
    "1. Dzielimy słowa na symbole ,,a l a END'' ,,m a m a END''  gdzie END jest symbolem końca wyrazu.\n",
    "2. Najczęstsza para obok siebie to ,,m a'' (20) razy\n",
    "3. Nowy symbol ,,ma''\n",
    "4. Nowy podział ,,a l a END'' ,,ma ma END''\n",
    "5. Najczęstsza para ,,ma ma'' (10) razy\n",
    "6. Nowy symbol ,,mama''\n",
    "7. Nowy podział ,,a l a END'' ,,mama END''\n",
    "8. itd.\n",
    "\n",
    "W pliku ,,brown_clusters.tsv'' pierwsza kolumna to identyfikator skupienia (nie używamy w tym zadaniu), druga kolumna to wyrazy, a trzecia to ich liczności w pewnym korpusie tweetów. Zaimplementuj technike BPE na tych słowach.\n",
    "\n",
    "Zaimplementuj algorytm BPE wykonujący `number_of_iterations` iteracji (łączeń symboli).\n",
    "\n",
    "[1] Sennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare words with subword units. In ACL 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ff3b90528fdb50de90c5c946c157e21",
     "grade": false,
     "grade_id": "cell-93d78a28d4e25cbc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "brown_df = pd.read_csv('brown_clusters.tsv', sep='\\t', header=0, names=['cluster', 'word', 'count'])\n",
    "\n",
    "number_of_iterations = 10\n",
    "def preform_bpe(brown_df, number_of_iterations):\n",
    "    \"\"\"\n",
    "    Funckcja przyjmuje ramkę w formacie analogicznym do obiektu brown_df (wczytany wyżej)\n",
    "     oraz liczbę iteracji.\n",
    "    Wyjściem funkcji powinna być lista słów z poszczególnymi tokenami/symbolami oddzielonymi spacją.\n",
    "    Za znak końca wyrazu przyjmij END. \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    rows = brown_df.dropna().iterrows()\n",
    "    results = []\n",
    "    \n",
    "    #################\n",
    "    # Split symbols\n",
    "    #################\n",
    "    for key, value in rows:\n",
    "        symbols = [*value['word']]\n",
    "        symbols += ['END']\n",
    "        # print(symbols)\n",
    "        results += [[symbols, value['count']]]\n",
    "    \n",
    "    #################\n",
    "    # Find most common pairs\n",
    "    #################\n",
    "\n",
    "    for _ in range(number_of_iterations):\n",
    "        most_common_pair = None\n",
    "        counter = Counter()\n",
    "        \n",
    "        for symbols, count in results:\n",
    "            for i, _ in enumerate(symbols[:-2]): # skip END\n",
    "                pair = (symbols[i], symbols[i+1])\n",
    "                # print(pair)\n",
    "                counter.update({pair: count})\n",
    "                \n",
    "        most_common_pair = counter.most_common(1)\n",
    "        \n",
    "        if not most_common_pair:\n",
    "            break\n",
    "        else:\n",
    "            most_common_pair = most_common_pair[0][0]\n",
    "            \n",
    "        print(most_common_pair)\n",
    "        \n",
    "        ##########################\n",
    "        # Replace and save tokens\n",
    "        ##########################\n",
    "        for i, (tokens, count) in enumerate(results):\n",
    "            raw_str = ' '.join(tokens)\n",
    "            if most_common_pair[0] + ' ' + most_common_pair[1] in raw_str:\n",
    "                tokens = raw_str.replace(most_common_pair[0] + ' ' + most_common_pair[1], most_common_pair[0] + most_common_pair[1]).split(' ')\n",
    "                results[i] = (tokens, count)\n",
    "    \n",
    "    return [' '.join(tokens) for tokens, _ in results]\n",
    "        \n",
    "# data = {'cluster': range(2), 'word':['ala', 'mama'], 'count': [5,10]}\n",
    "# df = pd.DataFrame (data, columns = ['cluster', 'word', 'count'])\n",
    "# preform_bpe(df, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test implementacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfff70f711bf389f0f1cd969e7c3a413",
     "grade": true,
     "grade_id": "cell-7e952fa8dcd136fe",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('m', 'a')\n"
     ]
    }
   ],
   "source": [
    "from nose.tools import assert_list_equal\n",
    "data = {'cluster': range(2), 'word':['ala', 'mama'], 'count': [5,10]}\n",
    "df = pd.DataFrame (data, columns = ['cluster', 'word', 'count'])\n",
    "vocab = preform_bpe(df, 1)\n",
    "assert_list_equal(vocab, ['a l a END', 'ma ma END'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spraw aby Twoja implementacja wypisywała kolejne łączone ze sobą symbole i uruchom Twoją funkcję na np. 50 iteracji, obserwując jakie tokeny są tworzone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'n')\n",
      "('t', 'h')\n",
      "('a', 'n')\n",
      "('e', 'r')\n",
      "('o', 'n')\n",
      "('o', 'u')\n",
      "('r', 'e')\n",
      "('.', '.')\n",
      "('a', 't')\n",
      "('t', 'o')\n",
      "('in', 'g')\n",
      "('i', 't')\n",
      "('th', 'e')\n",
      "('s', 't')\n",
      "('<', '@')\n",
      "('<@', 'M')\n",
      "('<@M', 'E')\n",
      "('<@ME', 'N')\n",
      "('<@MEN', 'T')\n",
      "('<@MENT', 'I')\n",
      "('<@MENTI', 'O')\n",
      "('<@MENTIO', 'N')\n",
      "('<@MENTION', '>')\n",
      "('m', 'e')\n",
      "('o', 'r')\n",
      "('l', 'l')\n",
      "('i', 's')\n",
      "('e', 'n')\n",
      "('a', 'r')\n",
      "('l', 'e')\n",
      "('y', 'ou')\n",
      "('o', 'w')\n",
      "('h', 'a')\n",
      "('c', 'o')\n",
      "('a', 'y')\n",
      "('s', 'e')\n",
      "('<', 'U')\n",
      "('<U', 'R')\n",
      "('<UR', 'L')\n",
      "('<URL', '-')\n",
      "('v', 'e')\n",
      "('b', 'e')\n",
      "('an', 'd')\n",
      "('l', 'o')\n",
      "('c', 'h')\n",
      "('e', 's')\n",
      "('e', 'd')\n",
      "('a', 's')\n",
      "('o', 'f')\n",
      "('g', 'o')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\\\ i END',\n",
       " '/ i / END',\n",
       " 'to d ay - i END',\n",
       " 'n ow i END',\n",
       " '# you e ver END',\n",
       " 'i f in a ll y END',\n",
       " '「 i END',\n",
       " '- i - END',\n",
       " 'in e v a END',\n",
       " '» i END',\n",
       " 'w hat t ay a END',\n",
       " 'i i i i i i i i i i END',\n",
       " '\\ue6d1 END',\n",
       " 'i k in d a END',\n",
       " 'lo l - i END',\n",
       " 'i a c t u a ll y END',\n",
       " 'w a d d y a END',\n",
       " '# as lon g as you END',\n",
       " 'd o you END',\n",
       " '\\u200e \\u200b i END',\n",
       " 'i ̇ END',\n",
       " 'ï END',\n",
       " '# lo l at g i r l s w h o END',\n",
       " '# r t i f you END',\n",
       " 'i j st END',\n",
       " '« i END',\n",
       " '• i END',\n",
       " 'w h o d a END',\n",
       " 'w ha d y a END',\n",
       " ') i END',\n",
       " '+ i END',\n",
       " '# you r f a c e m a k es me END',\n",
       " 'i i i i i i i i END',\n",
       " '` i END',\n",
       " 'i i i i i i i END',\n",
       " 'i a l re a d y END',\n",
       " '_ i END',\n",
       " '# you m a k e me END',\n",
       " '* i END',\n",
       " '| i END',\n",
       " '# u r b o y f r i en d e ver END',\n",
       " 'w h en i END',\n",
       " 'ι END',\n",
       " \"d on ' t cha END\",\n",
       " \"w h o ' d a END\",\n",
       " 'd you END',\n",
       " 'w ha d d ay a END',\n",
       " 'i on l y END',\n",
       " 'i j u s s END',\n",
       " 'i a l w ay s END',\n",
       " 'i i i i i END',\n",
       " 'd on cha END',\n",
       " '( i END',\n",
       " \"d ' y a END\",\n",
       " 'ı END',\n",
       " '# u e ver END',\n",
       " 'in e ver END',\n",
       " 'i - i END',\n",
       " 'i j u s END',\n",
       " '/ / i END',\n",
       " 'ist i ll END',\n",
       " 'w ha d d y a END',\n",
       " \"d ' you END\",\n",
       " 'i re a ll y END',\n",
       " 'd on t cha END',\n",
       " 'i j u st END',\n",
       " 'i END',\n",
       " '- i END',\n",
       " 'i you END',\n",
       " '# in n ow ay s ha p e or f or m END',\n",
       " '( you END',\n",
       " '/ / w e END',\n",
       " '/ / u END',\n",
       " '# men m ar r y w o men that END',\n",
       " '/ w e END',\n",
       " 'se l f - ed u c at i on END',\n",
       " '# re a l g r and m as END',\n",
       " '/ you END',\n",
       " '# s h ou tou t to g i r l s w h o END',\n",
       " '# b o y s w h o END',\n",
       " 'i / w e END',\n",
       " '# s h ou tou t to the g u y sthat END',\n",
       " '/ / you END',\n",
       " '# i lo ve p e o p le that END',\n",
       " '# n o t a ll b l a c k p e o p le END',\n",
       " '# i c an t st and p e o p le that END',\n",
       " '# s h ou tou t to the g i r l sthat END',\n",
       " '- the y END',\n",
       " '- w e END',\n",
       " '# h ow m an y p e o p le END',\n",
       " '- you END',\n",
       " 'w e END',\n",
       " '# a q u ar i an s END',\n",
       " 't the y END',\n",
       " 'th w y END',\n",
       " 'g u i l d en st er n END',\n",
       " \"d ' u END\",\n",
       " '# i hat e m a les w h o END',\n",
       " 't e h y END',\n",
       " 'th r y END',\n",
       " 'i f you END',\n",
       " '# h ou se h i p p o s END',\n",
       " 'the u END',\n",
       " 'the e y END',\n",
       " '# i hat e f e m a les w h o END',\n",
       " 'the y y END',\n",
       " 'the y END',\n",
       " 'v i o le t s END',\n",
       " 'e h o END',\n",
       " 'w h o ’ d END',\n",
       " 'w h o t f END',\n",
       " 'w h o ’ ve END',\n",
       " 'w h o d END',\n",
       " '<URL- re a l . co m > END',\n",
       " '# i l i k e p e o p le w h o END',\n",
       " '- w h o END',\n",
       " 'w h 0 END',\n",
       " 'w h u END',\n",
       " 'w h o END',\n",
       " \"w h o ' ve END\",\n",
       " 's s h e END',\n",
       " 'ser - u e ber w a ch er END',\n",
       " 's h h e END',\n",
       " 't est ast er is k END',\n",
       " '# m y d u m b as s END',\n",
       " 's j e END',\n",
       " 't a ch o m ast er END',\n",
       " 'i a l m o st END',\n",
       " 'i d on e END',\n",
       " '# w hat i f i END',\n",
       " 'h e / s h e / it END',\n",
       " '$ h e END',\n",
       " '# w hat i f go d END',\n",
       " '# i h e ar d ch u c k n or r is END',\n",
       " '# f m h 2 0 1 1 END',\n",
       " '# i h e ar d b ow w ow END',\n",
       " 'b l d _ 6 0 0 _ k w h END',\n",
       " 'b l d _ 6 5 0 _ k w h END',\n",
       " 's h e e e END',\n",
       " '# f m 2 0 1 1 END',\n",
       " '- s h e END',\n",
       " '- h e END',\n",
       " 's / h e END',\n",
       " 's h e / h e END',\n",
       " '- it END',\n",
       " 's h e e END',\n",
       " 'h e / s h e END',\n",
       " 'h e END',\n",
       " 's h e END',\n",
       " '<URL- i . ve > END',\n",
       " \"l ' ve END\",\n",
       " 'the y v END',\n",
       " \"you \\\\ ' ve END\",\n",
       " \"i ' be END\",\n",
       " '<URL- i h e ar t m o v i es . or g > END',\n",
       " 'i w ou l d a END',\n",
       " 'w e ` ve END',\n",
       " \"i ha ven ' t END\",\n",
       " '# i ha ven e ver END',\n",
       " \"a : i ' ve END\",\n",
       " 'w e v END',\n",
       " 'w e ´ ve END',\n",
       " \"y u ' ve END\",\n",
       " 'u ’ ve END',\n",
       " \"- i ' ve END\",\n",
       " \"there ' ve END\",\n",
       " \"i ' d a END\",\n",
       " '<URL- v o o m a x er . co m > END',\n",
       " \"that ' ve END\",\n",
       " \"w e ' v END\",\n",
       " 'you ´ ve END',\n",
       " 'i ve e END',\n",
       " \"i ' d ' ve END\",\n",
       " 'you ` ve END',\n",
       " \"i \\\\ ' ve END\",\n",
       " 'you v END',\n",
       " \"you ' v END\",\n",
       " '# n e ver ha ve i e ver END',\n",
       " \"u ' v END\",\n",
       " '<URL- n a u g h t y d o g . co m > END',\n",
       " 'i ha ven t END',\n",
       " \"i v ' e END\",\n",
       " 'the y ’ ve END',\n",
       " '# ha ve you e ver END',\n",
       " 'i ´ ve END',\n",
       " 'i ` ve END',\n",
       " '# ha ve u e ver END',\n",
       " 'the y ve END',\n",
       " \"i ' v END\",\n",
       " '0 . 0 0 % END',\n",
       " 'w e ve END',\n",
       " 'u ve END',\n",
       " 'w e ’ ve END',\n",
       " \"u ' ve END\",\n",
       " 'you ’ ve END',\n",
       " 'you ve END',\n",
       " 'i ’ ve END',\n",
       " \"the y ' ve END\",\n",
       " \"i ' ve END\",\n",
       " 'i ve END',\n",
       " \"you ' ve END\",\n",
       " \"w e ' ve END\",\n",
       " 'n o o o o o o t END',\n",
       " 'n o t t t t t t t END',\n",
       " 'n o h t END',\n",
       " 'you ha ve END',\n",
       " '/ n o t / END',\n",
       " 'n o it END',\n",
       " '- n o t - END',\n",
       " 'n o t t t t t t END',\n",
       " 'n o o o o o t END',\n",
       " \"n ' t END\",\n",
       " 'n n o t END',\n",
       " 'n a h t END',\n",
       " '_ n o t _ END',\n",
       " 'd eser ved l y END',\n",
       " 'n o t t t t t END',\n",
       " 'n o o o o t END',\n",
       " 'n o t - END',\n",
       " 'n o o o t END',\n",
       " 'n o o t END',\n",
       " 'n to END',\n",
       " 'n o t t t t END',\n",
       " 'n o t t t END',\n",
       " 'r i g h t f u ll y END',\n",
       " 'n a w t END',\n",
       " 'n 0 t END',\n",
       " 'n o t t END',\n",
       " 'n o t END',\n",
       " 'n t END',\n",
       " 'go t t n END',\n",
       " 'b 3 3 n END',\n",
       " 'be e e e e en END',\n",
       " 'go t ton END',\n",
       " 's u c c es s f u l y END',\n",
       " 'be e e e en END',\n",
       " 'be en n END',\n",
       " 'u n d er gon e END',\n",
       " 'be e e en END',\n",
       " 'be e en END',\n",
       " 'be en END',\n",
       " 'go t t en END',\n",
       " 'j u x t END',\n",
       " '/ / j u st END',\n",
       " 'j u st t t t t END',\n",
       " 'j x t END',\n",
       " '# s p or c le END',\n",
       " 'j st t END',\n",
       " 'j u r t END',\n",
       " 'j y s END',\n",
       " '/ j u st END',\n",
       " '- j u s END',\n",
       " 'j y st END',\n",
       " 'd d e u be l END',\n",
       " 'j u st t t t END',\n",
       " 'j u s s s st END',\n",
       " 'j u $ t END',\n",
       " 'j u u s END',\n",
       " 'j s st END',\n",
       " 'j u u u u u st END',\n",
       " 'k u st END',\n",
       " 'j h u s END',\n",
       " 'j u s s s s END',\n",
       " 'j h u s s END',\n",
       " 'j u st s END',\n",
       " 'j u s s st END',\n",
       " 'j u s x END',\n",
       " 'j u x x END',\n",
       " 'j z t END',\n",
       " 'j u z z END',\n",
       " 'j u d t END',\n",
       " '<URL- w o o . l y > END',\n",
       " 'j u h s END',\n",
       " 'j u u u u st END',\n",
       " 'j j u st END',\n",
       " 'j u z t END',\n",
       " 'j u st t t END',\n",
       " 'j u st ed END',\n",
       " 'j u s r END',\n",
       " 'j u u u st END',\n",
       " 'j u t s END',\n",
       " 'j u s y END',\n",
       " 'j u u st END',\n",
       " 'j u at END',\n",
       " 'j u s z END',\n",
       " '# j u st END',\n",
       " 'j s s END',\n",
       " 'j u s s s END',\n",
       " '<URL- b u y t t er . co m > END',\n",
       " 'j u s st END',\n",
       " 'j z END',\n",
       " '- j u st END',\n",
       " '# d on t a c t l i k e you n e ver END',\n",
       " 'j u t END',\n",
       " 'j u st t END',\n",
       " 'j u x END',\n",
       " 'j s u t END',\n",
       " 'j u z END',\n",
       " 'j st END',\n",
       " 'j u s s END',\n",
       " 'j u st END',\n",
       " 'j u s END',\n",
       " \"a in \\\\ ' t END\",\n",
       " 'a in ´ t END',\n",
       " \"a ' in t END\",\n",
       " 'a it n END',\n",
       " 'a in y END',\n",
       " 'w u s z END',\n",
       " 'ay n t END',\n",
       " 'a in n t END',\n",
       " 'i an t END',\n",
       " \"an ' t END\",\n",
       " 'a in e END',\n",
       " 'a in ` t END',\n",
       " 'a in n END',\n",
       " 'a in t t END',\n",
       " 'a i in t END',\n",
       " 'i a in t END',\n",
       " 'a in ’ t END',\n",
       " 'an it END',\n",
       " 'a in END',\n",
       " 'a in t END',\n",
       " \"a in ' t END\",\n",
       " 's h ou d a END',\n",
       " 's h ou l d n a END',\n",
       " 's h ou l a END',\n",
       " \"w ou l d n ' t ' ve END\",\n",
       " \"s h ou l d n ' t ' ve END\",\n",
       " 's h ou l d v END',\n",
       " \"s h u d ' ve END\",\n",
       " 'han t END',\n",
       " \"ha v ' n t END\",\n",
       " 'c l d a END',\n",
       " 'w l d ve END',\n",
       " \"s h ou l d ' a END\",\n",
       " 'w ou l d a a END',\n",
       " 's h ou l d d a END',\n",
       " 'w u l d ve END',\n",
       " \"w u d ' ve END\",\n",
       " 'w ou l d d a END',\n",
       " 's h u l d ve END',\n",
       " 's h ou l d a a END',\n",
       " \"ha ve ' t END\",\n",
       " 'cou l d ’ ve END',\n",
       " 'ha ven t t END',\n",
       " 's h l d ve END',\n",
       " 'c u d ve END',\n",
       " \"m ay ' ve END\",\n",
       " \"h v n ' t END\",\n",
       " 'w ou l d ’ ve END',\n",
       " 'a v n t END',\n",
       " 'w l d a END',\n",
       " 's h ou l d ’ ve END',\n",
       " 'c u l d a END',\n",
       " 'ha ven ´ t END',\n",
       " 's h l d a END',\n",
       " 'm i g h t ve END',\n",
       " 'ha ven ` t END',\n",
       " 'ha d n ’ t END',\n",
       " '# g lo c a l u r b an END',\n",
       " 'h ven t END',\n",
       " 's h u d ve END',\n",
       " 'w u d ve END',\n",
       " \"ha ve ' n t END\",\n",
       " 'c u d d a END',\n",
       " 'm i g h t a END',\n",
       " 'w u l d a END',\n",
       " 's h u l d a END',\n",
       " 'w u d d a END',\n",
       " 's h u d d a END',\n",
       " 'w u d a END',\n",
       " 's h u d a END',\n",
       " 'm u st ve END',\n",
       " 'h v n t END',\n",
       " \"m i g h t ' ve END\",\n",
       " 'ha d n t END',\n",
       " \"ha v n ' t END\",\n",
       " 'ha ven ’ t END',\n",
       " 'cou l d ve END',\n",
       " 'm u st a END',\n",
       " \"m u st ' ve END\",\n",
       " 'w ou l d ve END',\n",
       " 's h ou l d ve END',\n",
       " 'ha v n t END',\n",
       " 'cou l d a END',\n",
       " \"cou l d ' ve END\",\n",
       " 'w ou l d a END',\n",
       " \"ha d n ' t END\",\n",
       " \"s h ou l d ' ve END\",\n",
       " \"w ou l d ' ve END\",\n",
       " 's h ou l d a END',\n",
       " \"ha ven ' t END\",\n",
       " 'ha ven t END',\n",
       " 'n e v v a END',\n",
       " 'n e e e e ver END',\n",
       " 'n e ve t END',\n",
       " 'n e e e ver END',\n",
       " 'en ver END',\n",
       " 'n er ver END',\n",
       " 'n e e ver END',\n",
       " 'n e v a a a END',\n",
       " 'be ver END',\n",
       " '# in e ver END',\n",
       " 'g l a d y END',\n",
       " 'n e ve er END',\n",
       " '- n e ver END',\n",
       " \"n e ' er END\",\n",
       " 'le t cha END',\n",
       " 'le t ch u END',\n",
       " 'n e ver r r r END',\n",
       " 'n v a END',\n",
       " 'n e v a h END',\n",
       " 'n e v a a END',\n",
       " 'n e ver r r END',\n",
       " 'n ver END',\n",
       " 'n e ver r END',\n",
       " '# n e ver END',\n",
       " 'n e v r END',\n",
       " 'g l a d l y END',\n",
       " 'n v r END',\n",
       " 'n e ver END',\n",
       " 'n e v a END',\n",
       " 'e v u r END',\n",
       " 'e v a a a a a END',\n",
       " 'e ve a END',\n",
       " 'e ve e e er END',\n",
       " 'e ver r r r r r r r r END',\n",
       " 'e ver r r r r r r r END',\n",
       " 'e ve e er END',\n",
       " 'e v a a a a END',\n",
       " 'e ve er END',\n",
       " 'n e v ar END',\n",
       " 'e ver r r r r r r END',\n",
       " 'e v a a a END',\n",
       " 'e v a a END',\n",
       " 'e ver r r r r r END',\n",
       " 'e ver r r r r END',\n",
       " 'e v a h END',\n",
       " 'e ver r r r END',\n",
       " 'e ver r END',\n",
       " 'e ver r r END',\n",
       " 'e v r END',\n",
       " 'e v ar END',\n",
       " 'e v a END',\n",
       " 'e ver END',\n",
       " 'on le END',\n",
       " 'in l y END',\n",
       " 'on le e END',\n",
       " 'on l u END',\n",
       " 'on y l END',\n",
       " 'on ll y END',\n",
       " 'on l t END',\n",
       " 'on l y y y END',\n",
       " 'o l n y END',\n",
       " '- on l y END',\n",
       " '0 n l y END',\n",
       " 'on l i i END',\n",
       " 'on y END',\n",
       " 'on l y y END',\n",
       " 'on l y END',\n",
       " 'on l i END',\n",
       " 'g e t 2 END',\n",
       " 'n e c c es ar i l y END',\n",
       " 'n e c c es s ar i l y END',\n",
       " 'e e e m END',\n",
       " 'e ver n END',\n",
       " 'n e ven END',\n",
       " 'le t e m END',\n",
       " 'e ven n n END',\n",
       " 'e ve b END',\n",
       " 'e e en END',\n",
       " 'e ve m END',\n",
       " 'e ve en END',\n",
       " '<URL- g t p 1 2 3 . co m > END',\n",
       " '- e ven END',\n",
       " \"1 0 x ' s END\",\n",
       " \"m a k e ' e m END\",\n",
       " \"le t ' e m END\",\n",
       " 'e ven n END',\n",
       " 'e e m END',\n",
       " 'e v n END',\n",
       " 'e ven END',\n",
       " 'n e c es s ar i l y END',\n",
       " 're e e e e e e a ll y END',\n",
       " 're a a a l y END',\n",
       " 're a ll ll ll ll ll y END',\n",
       " 'r l i END',\n",
       " 're a ll i e END',\n",
       " 're a ll ll l y y y END',\n",
       " 're e l i END',\n",
       " 'r 3 a ll y END',\n",
       " 're a ll ll y y END',\n",
       " 're a ll y - re a ll y END',\n",
       " 're l a l y END',\n",
       " 're a ll ll y y y y END',\n",
       " 'r i ll i END',\n",
       " 're a ll y re a ll y re a ll y END',\n",
       " '- re a ll y - END',\n",
       " 're a ll y y y y y y END',\n",
       " 're a a ll y y END',\n",
       " 'e a ll y END',\n",
       " 're e e a a a ll y END',\n",
       " 're a ll y re a ll y END',\n",
       " 're e a a ll y END',\n",
       " 'r re a ll y END',\n",
       " 're a a a a ll l y END',\n",
       " 're a ll u END',\n",
       " 're a a a a a a ll y END',\n",
       " '/ re a ll y / END',\n",
       " 're a l y y END',\n",
       " 're a a ll l y END',\n",
       " 're a ll ll ll ll l y END',\n",
       " 're a ll ll y y y END',\n",
       " 're a a a ll l y END',\n",
       " 'w e a ll y END',\n",
       " 're e e e e e a ll y END',\n",
       " 're a ll l y y y y END',\n",
       " 're ll i END',\n",
       " 'g en u in l y END',\n",
       " 're a ll t END',\n",
       " 're a ll i i END',\n",
       " 're a ll l y y END',\n",
       " 're a a l y END',\n",
       " 're a ll ll ll ll y END',\n",
       " '_ re a ll y _ END',\n",
       " 're a ll y y y y y END',\n",
       " 're a ll y 2 END',\n",
       " 's h o le END',\n",
       " 're a a a a a ll y END',\n",
       " 're e l y END',\n",
       " 're lle END',\n",
       " 're a ll l y y y END',\n",
       " 's h o l END',\n",
       " 're e a ll y END',\n",
       " 're e e e e a ll y END',\n",
       " 're a ll ll ll l y END',\n",
       " 'r i ll y END',\n",
       " 're a ll y y y y END',\n",
       " 're a a a a ll y END',\n",
       " 're a a a ll y END',\n",
       " 'r i l i END',\n",
       " 're e e a ll y END',\n",
       " 're a a ll y END',\n",
       " 're a ll ll ll y END',\n",
       " 're e e e a ll y END',\n",
       " 're a ll y y y END',\n",
       " 'r i l y END',\n",
       " 's h o ll END',\n",
       " 're a l i END',\n",
       " 're l i END',\n",
       " 're a ll ll l y END',\n",
       " 're ll y END',\n",
       " 're a ll i END',\n",
       " 're le END',\n",
       " 're a ll y y END',\n",
       " 're a ll ll y END',\n",
       " 're a ll l y END',\n",
       " 'r ll y END',\n",
       " 'g en u in e l y END',\n",
       " 're a l y END',\n",
       " 're a ll y END',\n",
       " 'r l y END',\n",
       " 'a l red ay END',\n",
       " 'a l re ay d END',\n",
       " 'f in a l i END',\n",
       " 'of f is h END',\n",
       " 'a l r a d y END',\n",
       " 'w o o d a END',\n",
       " 'ored i END',\n",
       " 'a l re a a d y END',\n",
       " 'a l re a d y y y y y END',\n",
       " 'a l re a d d y END',\n",
       " 'a le a d y END',\n",
       " 'a l re ay END',\n",
       " 's u c es s f u ll y END',\n",
       " 'a l red i END',\n",
       " 'are a d y END',\n",
       " 'a l re a d i END',\n",
       " 'a l re a d i i END',\n",
       " 'a l re a d END',\n",
       " 'a l re a d y y y END',\n",
       " 'a w re a d y END',\n",
       " 'a l r d END',\n",
       " 'c u d a END',\n",
       " 'a l red y END',\n",
       " 'a ll re a d y END',\n",
       " 'a l re a d y y END',\n",
       " 'a l r d y END',\n",
       " 'p re v i ou s l y END',\n",
       " 'a l re a d y END',\n",
       " 're c en t l y END',\n",
       " '- a l m o st END',\n",
       " 'n e a l y END',\n",
       " 'n e ar ll y END',\n",
       " 'a l m 0 st END',\n",
       " 'a lo m o st END',\n",
       " 'a lo m st END',\n",
       " 'a l m o st t END',\n",
       " 'a l m s o t END',\n",
       " 'a m o st END',\n",
       " 'a ll m o st END',\n",
       " 'a l m st END',\n",
       " 'a ver a g ing END',\n",
       " 'r ou g h l y END',\n",
       " 'v i r t u a ll y END',\n",
       " 'a p p r o x i m at e l y END',\n",
       " 'p r a c t i c a ll y END',\n",
       " 'a l m o st END',\n",
       " 'n e ar l y END',\n",
       " 's i b b y END',\n",
       " 'c u r ren t y END',\n",
       " 'of f i c a i ll y END',\n",
       " 'of f c i a ll y END',\n",
       " '# b g g p l ay END',\n",
       " 'c u ren t l y END',\n",
       " 'b u s i l y END',\n",
       " 'of f i c a l y END',\n",
       " 'of i c i a ll y END',\n",
       " 'cor d i a ll y END',\n",
       " '<URL- l ist en . g h e t tor a d i o . f m > END',\n",
       " '<URL- k a isere g g . ch > END',\n",
       " 'h u c k le ber r i es END',\n",
       " 'h e ise END',\n",
       " '# re a d c ast END',\n",
       " 'p resen t l y END',\n",
       " 'of f i c i a l y END',\n",
       " '<URL- go . n i k e . co m > END',\n",
       " 'of f i c a ll y END',\n",
       " 're p or t ed l y END',\n",
       " 'of f i c i a ll y END',\n",
       " 'c u r ren t l y END',\n",
       " 'f in a ll l y y END',\n",
       " 'f in a ll i END',\n",
       " 'f in a ll l y y y END',\n",
       " '# of f i c i a ll y END',\n",
       " 'f i i i in a ll y END',\n",
       " 'f n a ll y END',\n",
       " 'f in a ll y y y y y END',\n",
       " 'f i in a ll y END',\n",
       " '- f in a ll y END',\n",
       " 'ber l y END',\n",
       " 'f in a ll ll ll y END',\n",
       " '# th ing s i d i d o ver thes u m mer END',\n",
       " '<URL- m a p m y f it n es s . co m > END',\n",
       " 'f in a ll ll l y END',\n",
       " 'f in i a ll y END',\n",
       " 's n a c k f e ed END',\n",
       " 'f in a ll y y y y END',\n",
       " 'f in a ll ll y END',\n",
       " 'f i an ll y END',\n",
       " '<URL- m a p m y r i d e . co m > END',\n",
       " 'f in a ll y y y END',\n",
       " 's u c c es f u ll y END',\n",
       " 'f in a ll y y END',\n",
       " '# m y f it n es s p a l END',\n",
       " 'f in n a ll y END',\n",
       " '<URL- m a p m y r u n . co m > END',\n",
       " 'f in a ll l y END',\n",
       " 're l u c t an t l y END',\n",
       " 'f in n a l y END',\n",
       " '<URL- co or d . in f o > END',\n",
       " 'f in a l y END',\n",
       " 'f in a ll y END',\n",
       " 's u c c es s f u ll y END',\n",
       " 'k n ow est END',\n",
       " 'r ath r END',\n",
       " 'c an st END',\n",
       " '<URL- s u p er m ar k e t . co m > END',\n",
       " 'r atha END',\n",
       " 'r ather END',\n",
       " 's ha l t END',\n",
       " \"d on t ' cha END\",\n",
       " 'i i on END',\n",
       " 'i m m o END',\n",
       " '4 + 4 END',\n",
       " 'i i b END',\n",
       " 'i d on t t END',\n",
       " 'b r in j a l END',\n",
       " 'i d on END',\n",
       " 'the ll END',\n",
       " '2 i END',\n",
       " 'n u st END',\n",
       " 'a b t a END',\n",
       " 'le y s END',\n",
       " 'me + you END',\n",
       " 'le m m a END',\n",
       " 's h ou l d s END',\n",
       " 'w e ll i END',\n",
       " 'i / i i END',\n",
       " 'd i d st END',\n",
       " '1 i END',\n",
       " 'char se t END',\n",
       " 'u l d END',\n",
       " 'd / n END',\n",
       " 'í END',\n",
       " 'f . i . n . a . l . s . END',\n",
       " 'i ow n END',\n",
       " 'i i d END',\n",
       " 'n on - v i r g in END',\n",
       " 's k y ha w k END',\n",
       " 's k y l an e END',\n",
       " 'ch _ t y p e END',\n",
       " 'k en o t END',\n",
       " 'd in n y END',\n",
       " '# i d on t END',\n",
       " '- i i END',\n",
       " '# y a m a m a e ver END',\n",
       " '2 0 1 0 / 0 7 END',\n",
       " '2 0 1 0 / 0 5 END',\n",
       " 'c . l . a . s . s . END',\n",
       " '& i END',\n",
       " 'i f u END',\n",
       " 'f m t END',\n",
       " 'that i END',\n",
       " 'le m i END',\n",
       " '# m y go a l f or 2 0 1 2 END',\n",
       " 'y u d END',\n",
       " 'e b u END',\n",
       " 'b o t t a END',\n",
       " 'm on e y - b a c k END',\n",
       " '1 9 t END',\n",
       " 'i 8 END',\n",
       " '# on l y f at p e o p le END',\n",
       " '# th ing s i a in t d on e y e t END',\n",
       " 'd on t ch u END',\n",
       " 's g e END',\n",
       " 'i f i END',\n",
       " 'i l d END',\n",
       " '# con f u s ing th ing s g i r l s d o END',\n",
       " '_ i _ END',\n",
       " 'm u z END',\n",
       " 'c an i END',\n",
       " '# u r g i r l f r i en d e ver END',\n",
       " 'i - i - i END',\n",
       " '# on l y u g l y p e o p le END',\n",
       " '2 0 1 0 / 1 2 END',\n",
       " 'le t t s END',\n",
       " 'n e er END',\n",
       " '2 0 1 0 / 1 0 END',\n",
       " '2 0 1 0 / 0 4 END',\n",
       " 'c y a a END',\n",
       " 's on t END',\n",
       " '2 0 1 0 / 0 8 END',\n",
       " 'd on n END',\n",
       " '2 0 1 0 / 0 6 END',\n",
       " 'a p t - g e t END',\n",
       " 'u on END',\n",
       " 'd o an END',\n",
       " 'd o st END',\n",
       " '# on l y w h it e p e o p le END',\n",
       " 'i i i i END',\n",
       " '# th ing s b l a c k p e o p led o END',\n",
       " 'i d w END',\n",
       " '2 + 2 END',\n",
       " 'in e END',\n",
       " 'hast END',\n",
       " 'i d i d n t END',\n",
       " '1 + 1 END',\n",
       " 'h ed END',\n",
       " 'p r o v o k ing END',\n",
       " 'i d n t END',\n",
       " 'u l END',\n",
       " 'u m a END',\n",
       " 'w d END',\n",
       " 'u d END',\n",
       " 'i i i END',\n",
       " 'll END',\n",
       " 'i v END',\n",
       " 'i on END',\n",
       " 'i i END',\n",
       " 'i d END',\n",
       " 'p r a c t i c a l y END',\n",
       " 'u s s u a ll y END',\n",
       " 'a c c i d en t i a ll y END',\n",
       " 's u d en l y END',\n",
       " 't e ar f u ll y END',\n",
       " 'u s u a ll l y END',\n",
       " 'n a i ve l y END',\n",
       " 're f le x i ve l y END',\n",
       " 'o p t i on a ll y END',\n",
       " 'l i k e 2 END',\n",
       " 'con t in u o s l y END',\n",
       " 'u n w ise l y END',\n",
       " 'p r a c t i c l y END',\n",
       " 'at u a ll y END',\n",
       " 'a c t l y END',\n",
       " 'p u r p o s l y END',\n",
       " 'se c re t e l y END',\n",
       " 'har d le y END',\n",
       " 'a u to m at i c a l y END',\n",
       " 'con c e i v a b l y END',\n",
       " 'a b sen t m in d ed l y END',\n",
       " 'a c t u a ll i END',\n",
       " 'l it r a ll y END',\n",
       " 's u b con c i ou s l y END',\n",
       " 'con st an l y END',\n",
       " 'l it er l y END',\n",
       " 'd o in t END',\n",
       " 'm a k e m END',\n",
       " 's u p p o se l y END',\n",
       " 'a c t u a ll y y END',\n",
       " 'a c t u l y END',\n",
       " 'go an n a END',\n",
       " 'b as c i a ll y END',\n",
       " 'p r at i c a ll y END',\n",
       " 'i i ve END',\n",
       " 's u p p o s a b l y END',\n",
       " 'b as i c a l y END',\n",
       " 'er r on e ou s l y END',\n",
       " 'f l at l y END',\n",
       " 'c as j END',\n",
       " 'le g it l y END',\n",
       " 'l it t er l y END',\n",
       " 'm u s s y END',\n",
       " 'or g in a ll y END',\n",
       " 'd on r END',\n",
       " 'in a d ver t an t l y END',\n",
       " 'a c t a ll y END',\n",
       " 'a c t u a l i END',\n",
       " 'u s u s a ll y END',\n",
       " 'a c c u a ll y END',\n",
       " 'in t u it i ve l y END',\n",
       " 'a u to m at i c l y END',\n",
       " 'g r u d g ing l y END',\n",
       " 'be g r u d g ing l y END',\n",
       " 's c ar c e l y END',\n",
       " 'b ar l y END',\n",
       " 'l it t er a l y END',\n",
       " 'b l at en t l y END',\n",
       " 'ha b it u a ll y END',\n",
       " 'or d in ar i l y END',\n",
       " 'a f f e c t i on at e l y END',\n",
       " 's n e a k i l y END',\n",
       " 'a c c i d en t a l y END',\n",
       " 'n or m a l y END',\n",
       " 's ing le hand ed l y END',\n",
       " 'co m p u l s i ve l y END',\n",
       " 's u b l i m in a ll y END',\n",
       " 'in v o l u n t ar i l y END',\n",
       " 'a c t u a ll l y END',\n",
       " 'g in e END',\n",
       " 'd e f in it i ve l y END',\n",
       " 'u s u a l y END',\n",
       " 'l it er a l y END',\n",
       " 'b r a ve l y END',\n",
       " 'a c c t u a ll y END',\n",
       " 'a c u a ll y END',\n",
       " 'b as i c l y END',\n",
       " 'ha f f END',\n",
       " 'in st in c t i ve l y END',\n",
       " 'u n con s c i ou s l y END',\n",
       " 's ing le - hand ed l y END',\n",
       " 's l y l y END',\n",
       " 'a c t a u ll y END',\n",
       " 'd r u n k en l y END',\n",
       " 'a c u t a ll y END',\n",
       " 'f o o l is h l y END',\n",
       " 'be ar l y END',\n",
       " 'p u r p o se f u ll y END',\n",
       " 'j o k ing l y END',\n",
       " 'r ou t in e l y END',\n",
       " 'k n ow ing l y END',\n",
       " 's u b con s c i ou s l y END',\n",
       " 'u n k n ow ing l y END',\n",
       " 'a c t u ll y END',\n",
       " 'm i r a c u lou s l y END',\n",
       " 'l it t er a ll y END',\n",
       " 'co lle c t i ve l y END',\n",
       " 't r a d it i on a ll y END',\n",
       " 'in a d ver t en t l y END',\n",
       " 'm ist a k en l y END',\n",
       " 'v o l u n t ar i l y END',\n",
       " 'b l in d l y END',\n",
       " 'in d i re c t l y END',\n",
       " 's p on t an e ou s l y END',\n",
       " 'w i ll ing l y END',\n",
       " 'h ere b y END',\n",
       " 'g r a d u a ll y END',\n",
       " 'a c t u a l y END',\n",
       " 'j es END',\n",
       " 'd e l i ber at e l y END',\n",
       " 'con t in u ou s l y END',\n",
       " 'se l d o m END',\n",
       " 'in t en t i on a ll y END',\n",
       " 'p u r p o se l y END',\n",
       " 'b ar le y END',\n",
       " 'in it i a ll y END',\n",
       " 'a c t i ve l y END',\n",
       " 'on t END',\n",
       " 'c as u a ll y END',\n",
       " 'es sen t i a ll y END',\n",
       " 'p r ou d l y END',\n",
       " 't y p i c a ll y END',\n",
       " 'a c c i d en t l y END',\n",
       " 'm a g i c a ll y END',\n",
       " 'a lle g ed l y END',\n",
       " 's u p p o sed l y END',\n",
       " 'or i g in a ll y END',\n",
       " 'se c re t l y END',\n",
       " 'g en er a ll y END',\n",
       " 'r are l y END',\n",
       " 'a u to m at i c a ll y END',\n",
       " 'a c c i d en t a ll y END',\n",
       " 'r and o m l y END',\n",
       " 'n or m a ll y END',\n",
       " 'con st an t l y END',\n",
       " 'har d l y END',\n",
       " 'b are l y END',\n",
       " 'b as i c a ll y END',\n",
       " 'l it er a ll y END',\n",
       " 'a c t u a ll y END',\n",
       " 'u s u a ll y END',\n",
       " 's u d d en t l y END',\n",
       " '- a l w ay s END',\n",
       " 'a lo s END',\n",
       " 'co in c i d en t l y END',\n",
       " 'p r ay er f u ll y END',\n",
       " 'd e m on b r u en END',\n",
       " 'd es p ar at e l y END',\n",
       " 'g i es END',\n",
       " 'd es p er at l y END',\n",
       " 'as lo END',\n",
       " 'a l t er n at e l y END',\n",
       " 'be l at ed l y END',\n",
       " 'j x END',\n",
       " 's u b se q u en t l y END',\n",
       " 's in c er l y END',\n",
       " 'h en c e f or th END',\n",
       " 'u l t i m at e l y END',\n",
       " 'd es p er at e l y END',\n",
       " 's in c ere l y END',\n",
       " 's u d d en l y END',\n",
       " 'a l s o END',\n",
       " 'o b i ou s l y END',\n",
       " 'd e a d a z z END',\n",
       " 'e v i d en t a ll y END',\n",
       " 'to t a ll y y y END',\n",
       " 's r i ou s l y END',\n",
       " 'o b v i ou l s y END',\n",
       " 'o b v s l y END',\n",
       " 'ser o i u s l y END',\n",
       " 'n e ver r r r r r r END',\n",
       " 'ser is ou l y END',\n",
       " 'h on est l y y END',\n",
       " 's r l s y END',\n",
       " 'ser i ou s l y y y y END',\n",
       " 'l it er a ll l y END',\n",
       " 'o b v i o s l y END',\n",
       " 'o v i ou s l y END',\n",
       " 'go t t cha END',\n",
       " 'o b v z END',\n",
       " 'd / a END',\n",
       " 'w is h i END',\n",
       " 'go t cha a END',\n",
       " 'i i i i i i i i i END',\n",
       " '# j u st c a u se w e co o l END',\n",
       " \"s or r y ' s END\",\n",
       " 'l i k e e e e e e END',\n",
       " 's h e e e e END',\n",
       " 'go t ch y a END',\n",
       " 's r l y END',\n",
       " 'f er re a l END',\n",
       " 'ser i ou s l y y y END',\n",
       " 's i r i u s l y END',\n",
       " 'g e z END',\n",
       " 'n e ver r r r r r END',\n",
       " 'lo k e y END',\n",
       " 's u r le y END',\n",
       " 'ser i ou s ll y END',\n",
       " 'h on es l t y END',\n",
       " 'ser i u o s l y END',\n",
       " 'd e a d as s s END',\n",
       " 'low k e y y END',\n",
       " '- re a ll y END',\n",
       " 'ser i u s l y END',\n",
       " 'ser i ou l y END',\n",
       " 'be t ch u END',\n",
       " 'ser ou s l y END',\n",
       " 'ser z l y END',\n",
       " 'h i g h k e y END',\n",
       " 'n e ver r r r r END',\n",
       " 'ser i o s l y END',\n",
       " 'l i k e e e e e END',\n",
       " 'ser i ou l s y END',\n",
       " 'ser i o s u l y END',\n",
       " 'p er s on a l y END',\n",
       " 'u n d er st and a b l y END',\n",
       " 'u n n o END',\n",
       " 'ser i ou s l y y END',\n",
       " 'g ed d it END',\n",
       " 'l i k e e e e END',\n",
       " 'the ore t i c a ll y END',\n",
       " 'd . a END',\n",
       " 're a l ist i c a ll y END',\n",
       " 'o b v i END',\n",
       " 't r u th f u ll y END',\n",
       " 'o b v s END',\n",
       " 'be t cha END',\n",
       " '# low k e y END',\n",
       " 'o b v END',\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preform_bpe(brown_df, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9888b25499797c4fb0fd4f13646b0c3c",
     "grade": false,
     "grade_id": "cell-7d1e49878db56df4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Jakie angielskie słowo jako pierwsze dostało swój własny token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df4c7b8b5aa2b077eaa2d42429797139",
     "grade": true,
     "grade_id": "cell-acd48c77e2c1bcec",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd51e6fc0cd1d3b4d8b9e9a2fa1b0316",
     "grade": false,
     "grade_id": "cell-df60f5e5c6fe4ca0",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Jakie są zalety korzystania z tokenizacji BPE w kontekście tworzenia reprezentacji (problem OOV, odnieś się do  k-gramów i n-gramów)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64306e36b58f1eee12c8bb339123e105",
     "grade": true,
     "grade_id": "cell-006ef6fd3e397206",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10\n",
    "\n",
    "W przypadku BPE mamy większe szanse na znalezienie znaczenia słowa OOV poprzez dopasowanie części wyrazu (np. w języku niemieckim: jeśli znamy znaczenie 'fahre' to 'gefahren' będzie miało zapewne podobne znaczenie). K-gramy (czyli podciągi k-literowe) są mniejszym zbiorem od n-gramów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2 - klasyfikacja (15 pkt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy kod powinien wczytać i ztokenizować zbiór danych dot. analizy wydźwięku. Jeśli nie masz biblioteki `nltk` musisz ją zainstalować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data set ['tweets.txt']\n"
     ]
    }
   ],
   "source": [
    "from helpers import DataSet\n",
    "training_set = DataSet(['tweets.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej znajdziesz przykład odczytu jednego tweeta z obiektu DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.\n",
      "['dear', '@microsoft', 'the', 'newooffice', 'for', 'mac', 'is', 'great', 'and', 'all', ',', 'but', 'no', 'lync', 'update', '?', \"c'mon\", '.']\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "for i in training_set.tweets:\n",
    "    print(i.text)\n",
    "    print(i.tokens)\n",
    "    print(i.clazz)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systemy IL często pracują z bardzo dużą liczbą cech, które są rzadkie np. cechy Bag-Of-Words, cechy n-gramowe itd. Powoduje to że klasyczna macierz przykłady uczące na cechy rośnie do bardzo dużych rozmiarów nawet dla małych zbiorów uczących (w sensie liczby przykładów). Ponadto samo przechowywanie w pamięci słownika mapującego konkretne słowa/n-gramy na indeksy kolumn macierzy może być bardzo kosztowne pamięciowo przy dużych rozmiarach słownika.\n",
    "\n",
    "Istnieje jednak technika, która pozwala nam na ominięcie tej przeszkody: haszowanie cech. Opis tej techniki znajdziesz na stronie:  https://en.wikipedia.org/wiki/Feature_hashing Jest ona też implementowana w obiekcie `sklearn.feature_extraction.FeatureHasher`. Zapoznaj się z opisem techniki i wykonaj poniższe polecenia.\n",
    "\n",
    "- Wykorzystując haszowanie cech wytrenuj wybrany klasyfikator na zbiorze uczącym dla cech Bag-of-words (możesz też spróbować cechy n-gramowe). Możesz wykorzystać gotową tokenizację we właściwości `.tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac05ad71ee90b1c800030849c5321cb7",
     "grade": true,
     "grade_id": "cell-f6cfe39258fbec51",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "n_features=30\n",
    "\n",
    "y_test = [tweet.clazz for tweet in training_set.tweets[ :int(len(training_set.tweets) * 0.3)] ]\n",
    "train_tokens = [tweet.tokens for tweet in training_set.tweets[ int(len(training_set.tweets) * 0.3):] ]\n",
    "y_train = [tweet.clazz for tweet in training_set.tweets[ int(len(training_set.tweets) * 0.3):] ]\n",
    "\n",
    "test_tokens = []\n",
    "train_tokens = []\n",
    "test_classes = []\n",
    "train_classes = []\n",
    "\n",
    "\n",
    "def split_training_set(training_set):\n",
    "    size = len(training_set.tweets)\n",
    "    split_point = int(size * 0.3)\n",
    "    \n",
    "    # Testing set\n",
    "    for row in training_set.tweets[ : split_point ]:\n",
    "        test_tokens.append(row.tokens)\n",
    "        test_classes.append(row.clazz)\n",
    "    \n",
    "    # Training set\n",
    "    for row in training_set.tweets[ split_point : ]:\n",
    "        train_tokens.append(row.tokens)\n",
    "        train_classes.append(row.clazz)\n",
    "        \n",
    "        \n",
    "def hash_features(n_features, train_tokens, test_tokens):\n",
    "    feature_hasher = FeatureHasher(n_features=n_features, input_type=\"string\")\n",
    "    return feature_hasher.transform(train_tokens), feature_hasher.transform(test_tokens)\n",
    "\n",
    "\n",
    "def train(n_features, train_tokens, train_classes, test_tokens, test_classes):    \n",
    "    train_hashed, test_hashed = hash_features(n_features, train_tokens, test_tokens)\n",
    "    classifier = SVC(kernel=\"linear\")\n",
    "    classifier.fit(train_hashed, train_classes)\n",
    "    predictions = classifier.predict(test_hashed)\n",
    "    accuracy = accuracy_score(test_classes, predictions)\n",
    "    return classifier, accuracy\n",
    "\n",
    "split_training_set(training_set)\n",
    "classifier, accuracy = train(n_features, train_tokens, train_classes, test_tokens, test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd6bcaf8dae7184b60bd9a8adadd85d8",
     "grade": false,
     "grade_id": "cell-1caf16c401c91ef2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Stwórz wykres zależności wybranej miary klasyfikacji od wymiarów macierzy danych (chodzi o liczbę cech do których haszujemy cechy oryginalne). Wystarczy przetestować kilka (>=4) wybranych wartości na skali logarytmicznej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bd253bac561b269cff3a3dceadc70f0",
     "grade": true,
     "grade_id": "cell-8076c16242981ae9",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n_features = 2**np.arange(5, 9)\n",
    "accuracies = []\n",
    "for n in n_features:\n",
    "    _, accuracy = train(n, train_tokens, train_classes, test_tokens, test_classes)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "plt.xscale('log', basex=2)\n",
    "plt.xlabel('Feature count')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.scatter(dims, accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82f3f52a6fe2a10a300b5d45101b32b5",
     "grade": false,
     "grade_id": "cell-eab7c2a5f0251ff4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    " - Obserwując stworzony wykres - skomentuj. Jak dużo jakości klasyfikacji się traci (albo zyskuje?) korzystając z mniejszej liczby haszowanych cech? Często klasyfikatory bardzo dobrze działają nawet przy liczbie haszowanych cech dla których na pewno istnieją konflikty cech oryginalnych - jak myślisz dlaczego? (Pomyśl o interpretacji takich skonfliktowanych cech)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed30f2d487da41cf1a92ffb63195d621",
     "grade": true,
     "grade_id": "cell-2caea1821af5d8aa",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Im mniejsza ilość cech tym gorsze accuracy - w prezentowanym zakresie zmiana ~6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20139da166319b49eea5cc7e984fc08e",
     "grade": false,
     "grade_id": "cell-0d86672dbabbf54d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    " - W poprzednim zadaniu wczytałeś wynik grupowania Browna do pamięci. Wytrenuj klasyfikator na reprezentacji ,,Bag-of-clusters'' tj. w kolumnach zamiast słów/n-gramów będziesz miał grupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b13c0457af5dab17e12780eafb1c5ac4",
     "grade": true,
     "grade_id": "cell-55264f6fe514d007",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5100794135613927"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Prepare dict\n",
    "brown_kv = {}\n",
    "for key, value in brown_df.dropna().iterrows():\n",
    "    new_pair = {value['word'] : value['cluster']}\n",
    "    brown_kv.update(new_pair)\n",
    "\n",
    "\n",
    "def train_with_clusters(train_tokens, train_classes, test_tokens, test_classes):    \n",
    "    vectorizer = CountVectorizer(preprocessor=None, lowercase=False)\n",
    "    train_clusters, test_clusters = vectorize_tokens(train_tokens, tokens_test)\n",
    "    classifier = SVC(kernel=\"linear\")\n",
    "    classifier.fit(train_clusters, train_classes)\n",
    "    predictions = classifier.predict(test_clusters)\n",
    "    accuracy = accuracy_score(test_classes, predictions)\n",
    "    \n",
    "    return classifier, accuracy\n",
    "\n",
    "def to_clusters(data):        \n",
    "    all_clusters = []\n",
    "    for tokens in data:\n",
    "        tweet_clusters = []\n",
    "        for token in tokens:\n",
    "            try: \n",
    "                key = str(brown_kv[token])\n",
    "                tweet_clusters.append(key)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            \n",
    "        all_clusters.append(\" \".join(tweet_clusters))\n",
    "    return all_clusters\n",
    "\n",
    "def vectorize_tokens(train_tokens, test_tokens):    \n",
    "    vectorizer = CountVectorizer(preprocessor=None, lowercase=False)\n",
    "    return vectorizer.fit_transform(to_clusters(train_tokens)), vectorizer.transform(to_clusters(test_tokens))\n",
    "\n",
    "\n",
    "_, accuracy = train_with_clusters(train_tokens, train_classes, test_tokens, test_classes)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e47a053ebc12ac2fd97d9c11187da9b",
     "grade": false,
     "grade_id": "cell-493071698fc0205e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Podsumuj eksperymenty: poznałeś dwie możliwości ograniczenia liczby cech - zastąpienie słów ich grupami i haszowanie cech. Jakie są wady i zalety obydwu podejść?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b80ace505afba9b12fd5d3896a9046ef",
     "grade": true,
     "grade_id": "cell-4508400659f7243e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Grupowanie:\n",
    "- lepsza możliwość analizy słów spoza słownika\n",
    "- słowa wewnątrz grupy mają podobne znaczenie\n",
    "- trwa dłużej niż hashowanie\n",
    "\n",
    "Hashowanie:\n",
    "- szybsze niż grupowanie\n",
    "- brak konieczności przechowywania informacji o dopasowaniu słowa do grupy znaczeń\n",
    "\n",
    "W powyższych przykładach, accuracy było lepsze dla grupowania"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
